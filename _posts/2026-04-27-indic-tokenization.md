---
layout: distill
title: How many tokens does it take to say "नमस्ते"? A Dive into Indic Tokenization
description: Tokenizers trained on English-dominant data often produce unusually high token counts for Indic languages. This "tokenizer fertility" increases sequence lengths, raises compute costs, and can hurt downstream performance, even when the underlying model is strong. In this post, we examine how fertility varies across major Indic scripts and how it affects language modeling quality, inference efficiency, and instruction-following behavior.
date: 2026-04-27
future: true
htmlwidgets: true
hidden: true

mermaid:
  enabled: true
  zoomable: true

# Anonymize when submitting
authors:
  - name: Anonymous
    url: ""
    affiliations:
      name: Anonymous

bibliography: 2026-04-27-indic-tokenization.bib

toc:
  - name: What're tokens? and why do they matter so much to us?
    subsections:
      - name: Introduction
      - name: Tokenizer Fertility - A Key Measure of Tokenization Efficiency
  - name: The Indic Problem!
    subsections:
      - name: How high fertility tokenizers make IndicNLP unfair?
  - name: The Fertility "Tax"
  - name: The Downstream Impact

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---


## What're tokens? and why do they matter so much to us?

<div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;">
  <strong>TL;DR.</strong> Some of the important pointers we'll look at, in this post: Are tokenizers failing a billion Indic language speakers? How tokenization bias reinforces linguistic inequality in AI models?
</div>

### Introduction

In recent years, the field of Natural Language Processing (NLP) has been revolutionized by advances in Large Language Models (LLMs).<d-cite key="hagos2024recentadvancesgenerativeai"></d-cite> Trained on large text corpora, these AI models can understand, generate and manipulate language in a human-like way, enabling a wide range of tasks without task-specific supervision. Simply, an LLM takes an input (prompt) and generates the output. They’re like magic, no?

But how do they operate? How do they see strings? LLMs do not operate directly on raw text. Instead, when a text is fed into the model, before interpreting, it’s first segmented into a series of multi-letter chunks called `tokens`. The process of converting text into these units is called tokenization.

### Tokenizer Fertility - A Key Measure of Tokenization Efficiency

Most modern LLMs use subword tokenizers. These tokenizers learn a vocabulary of common text fragments from a large corpus. At inference time, each word is broken into the longest possible fragments from this vocabulary. For languages with predictable morphology or large amounts of training data (like English), this strategy works reasonably well. However, tokenization performance varies across languages. Now, if a tokenizer was mainly trained on high-resource languages, it might've failed to learn useful subword units for languages that were underrepresented in its training data. When this happens, a single word may split into many tokens. This effect is called high fertility.

**Tokenizer fertility** $(F)$ (a measure of how many tokens a model generates per source word) can be defined by:

$$\text{F}(L) = \frac{1}{|D_L|} \sum_{s \in D_L} \frac{\text{Count}_{\text{tokens}}(s)}{\text{Count}_{\text{words}}(s)}$$

where $|D_L|$ refers to the number of sentences in a dataset $D$ of language $L$ and $s$ is a sentence in the dataset.

Now, More tokens $\leadsto$ longer sequences, $\implies$ less context fits into the model's fixed window. Fragmented words give the model fewer stable patterns to learn, reducing sample efficiency.

Simply, you can consider tokens as the "units of thought" the model works with. If those units are poorly aligned with a language, the model begins at a disadvantage -- before any actual modeling even starts!

## The Indic Problem!

{% include figure.liquid path="assets/img/2026-04-27-indic-tokenization/banner.jpg" class="img-fluid" %}
<div class="caption">
    Credits: Generated by Gemini Nano Banana
</div>

[Indic languages](https://en.wikipedia.org/wiki/Indo-Aryan_languages) represent one of the world's largest and most diverse linguistic families, spoken by hundreds of millions across South Asia. The diversity of these languages reflects the philosophy of Vasudhaiva Kutumbakam – "the world is one family". Indic languages form a critical yet underserved segment of the NLP landscape. This reminds us that technology should serve all languages, not just the high-resource ones. Building language models that handle Indic languages effectively is a step toward more inclusive AI, ensuring that speakers of every language can benefit from advances in LLMs.

Training Data Dominance: Tokenization algorithms, such as Byte Pair Encoding (BPE), are primarily trained on massive text corpora dominated by high-resource languages, especially English. The resulting vocabularies are optimized for the structure and script of these dominant languages.

### How high fertility tokenizers make IndicNLP unfair?

<div style="border: 2px solid #ee6b6e; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;">
  Consider "नमस्ते" (trans. namaste), meaning "Hello". When tokenized, it should be preserved as one meaningful token. But what if it was wrongly tokenized as ["Hell", "o"]? The semantic meaning is gone! This is the default reality for Indic languages :<
</div>

- Inefficient Segmentation: When a tokenizer breaks a single character into 3-4 tokens, the model loses the semantic unity of that character. It forces the model to learn character composition rather than word meaning.

Indic scripts overwhelm subword models! For non-Latin-script and morphologically complex or agglutinative languages where words are formed by joining many morphemes (for eg. Dravidian language families), the models struggle to create meaningful tokens as their word forms change frequently and the tokenizer does not capture these variations well. These languages often require significantly more tokens to represent the same semantic content as English. For example, some languages may require up to seven times more tokens per sentence than English. We’ll look into this soon.

- Computational Burden: The token inflation leads to higher computational costs and slower processing times. This creates a systematic disadvantage and an accessibility barrier for speakers of these Indic languages. In commercial AI services that use token-based pricing, Indic languages face disproportionately higher costs for the same task, creating economic barriers to accessing AI technology.

- Reduced Context Utilization: Higher token density means less effective use of a model's fixed context window, which can impair performance on tasks requiring extensive contextual understanding.

Let's look at an example using [OpenAI Tokenizer Playground](https://platform.openai.com/tokenizer). "Day by day, it seems nothing changes, yet soon, everything is different.". A translation of this sentence in Bangla (keeping the unicode codepoint count same) would be "দিনে দিনে মনে হয় কিছুই বদলায় না, কিন্তু খুব শিগগিরই সবকিছুই বদলে যায়।".

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/2026-04-27-indic-tokenization/gpt-en.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/2026-04-27-indic-tokenization/gpt-bn.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Tokenization in GPT-4o which uses a form of Byte Pair Encoding (BPE) via OpenAI's `tiktoken` library, specifically a highly optimized, large-vocabulary version (`o200k_base`).
</div>

Look at the difference in tokenization. Now, this is for just one sentence (again, this worsens for more complex Languages like Malayalam), imagine a huge dataset with thousands of rows to process for inference – these small differences quickly add up, increasing sequence lengths, affecting model memory and may ultimately impact performance.

---

Let's try some mini experiments.

How Fertile Is Your Tokenizer? Visualising the splits:

{% highlight python %}
from transformers import AutoTokenizer
import pandas as pd

def visualize_splits(text, model_names):
    results = []
    for name in model_names:
        tokenizer = AutoTokenizer.from_pretrained(name)
        tokens = tokenizer.tokenize(text)
        
        split_view = " | ".join([t.replace('Ġ', '').replace(' ', '') for t in tokens])
        results.append({
            "Model": name.split("/")[-1],
            "Token Count": len(tokens),
            "Split View": split_view
        })
    return pd.DataFrame(results)

text = "संप्रभुता" 

models = [
    "microsoft/Phi-3-mini-4k-instruct",
    "google/gemma-2-2b", 
    "ai4bharat/indic-bert"
]

df = visualize_splits(text, models)
print(df.to_markdown(index=False))
{% endhighlight %}

To understand the source of high fertility, we tokenized the Hindi word for "Sovereignty" (संप्रभुता). This word serves as a stress test due to its use of conjuncts (yuktakshars) and vowel modifiers (matras).

| Model                  |   Token Count | Split View                        |
|:-----------------------|--------------:|:----------------------------------|
| Phi-3-mini-4k-instruct |            10 | ▁ | स | ं | प | ् | र | भ | ु | त | ा |
| gemma-2-2b             |             4 | सं | प्र | भु | ता                    |
| indic-bert             |             3 | ▁सप | रभ | त                      |

The difference in segmentation strategies is distinct:

Phi-3-mini (10 Tokens): Orthographic Decomposition - The tokenizer fails to recognize Indic subwords, reverting to character-level segmentation. Notably, it splits the conjunct 'प्र' (pra) into three constituent parts: the consonant प, the halant ्, and the consonant r र. The model essentially processes the text as a stream of unicode distincts rather than linguistic units.

Gemma-2 (4 Tokens): Syllabic Preservation - The tokenizer aligns with the structure of the script, preserving full syllables ("Aksharas"). The complex clusters प्र (pra) and भु (bhu) are treated as single tokens.

Coming to AI4Bharat's IndicBERT, the result (token count of 3) might seem great at first glance. However, if you look closely at the split view: सप (Sap), रभ (Rabh), त (Ta), you'll notice that the vowels have disappeared. The tokenizer has achieved this low fertility by performing aggressive normalization.

The Trade-off: The model is extremely efficient, but potentially loses critical semantic information (tense, gender, and root meaning) stored in the vowels. This serves as a crucial lesson: Low fertility is only a virtue if it preserves information.

---

The sensitivity test:

{% highlight python %}
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import numpy as np
import pandas as pd
import gc

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

def measure_fertility_perplexity(text, model_id):
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto"
    )

    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    token_count = inputs.input_ids.shape[1]

    with torch.no_grad():
        outputs = model(**inputs, labels=inputs.input_ids)
        perplexity = torch.exp(outputs.loss).item()

    del model
    gc.collect()
    torch.cuda.empty_cache()

    return {
        "Model": model_id.split("/")[-1],
        "Token Count": token_count,
        "Perplexity": round(perplexity, 2)
    }

#eng_text = "Artificial intelligence is transforming the world."
hindi_text = "कृत्रिम बुद्धिमत्ता दुनिया को बदल रही है।"

models = ["microsoft/Phi-3-mini-4k-instruct", "google/gemma-2-2b"]

results = [measure_fertility_perplexity(hindi_text, m) for m in models]

print(pd.DataFrame(results).to_markdown(index=False))
{% endhighlight %}

We hypothesized that high fertility would confuse the model, leading to higher perplexity (uncertainty). To test this, we compared Microsoft Phi-3 (English-centric tokenizer) against Google Gemma-2 (Multilingual tokenizer) on a Hindi sample. The results, at first glance, seem to defy logic:

| Model                  |   Token Count |   Raw Perplexity |
|:-----------------------|--------------:|-----------------:|
| Phi-3-mini-4k-instruct |            44 |             5.65 |
| gemma-2-2b             |            14 |            44.73 |

Does this mean the English-centric Phi-3 is 8x "smarter" at Hindi than the multilingual Gemma-2? Absolutely not. This is a classic example of the Tokenization Bias in evaluation metrics.

Perplexity measures the average uncertainty per token.

- Phi-3: Because it fragments the Hindi sentence into 44 tiny bytes, many of its prediction steps are trivial. For example, once it predicts the first byte of a character, the subsequent bytes are deterministic. These "easy wins" lower the average perplexity, masking the fact that the model may not grasp the sentence's semantic meaning.

- Gemma-2: With a richer vocabulary, Gemma represents the sentence in just 14 dense tokens. Each prediction requires choosing the correct word or root from a large set, so each step is harder. 

To compare them fairly, we normalize perplexity by word count, not token count.

$$\text{PPL}_{\text{word}} = \text{PPL}_{\text{token}}^{(\text{Token Count} / \text{Word Count})}$$

Thus, Phi-3 Normalized: $5.65^{(44/7)} \approx 5.65^{6.28} \approx \mathbf{52,800}$

Gemma-2 Normalized: $44.73^{(14/7)} \approx 44.73^{2.0} \approx \mathbf{2,000}$

**The Reality**: When normalized, Gemma-2 is actually orders of magnitude better at predicting the sequence than Phi-3.

---

## The Fertility "Tax"

## The Downstream Impact
